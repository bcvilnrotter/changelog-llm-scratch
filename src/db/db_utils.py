"""
Database utility functions for the changelog-llm project.
This module provides common database operations for the training process.
"""

import json
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import sqlite3

from src.db.db_schema import get_db_connection, init_db

def create_training_run(model_name: str, base_model: str, hyperparameters: Dict, git_commit: Optional[str] = None) -> int:
    """
    Create a new training run entry in the database.
    
    Args:
        model_name (str): Name of the model being trained
        base_model (str): Name of the base model being fine-tuned
        hyperparameters (dict): Training hyperparameters
        git_commit (str, optional): Git commit hash
    
    Returns:
        int: ID of the new training run
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    
    cursor.execute('''
        INSERT INTO training_runs (
            model_name, base_model, hyperparameters, git_commit, 
            status, timestamp, metrics
        ) VALUES (?, ?, ?, ?, ?, ?, ?)
    ''', (
        model_name, 
        base_model, 
        json.dumps(hyperparameters), 
        git_commit, 
        'running', 
        timestamp, 
        '{}'
    ))
    
    run_id = cursor.lastrowid
    conn.commit()
    conn.close()
    
    if run_id is None:
        return -1  # Return a sentinel value if no ID was generated
    return run_id

def update_training_run_status(run_id: int, status: str, metrics: Optional[Dict] = None) -> bool:
    """
    Update the status and metrics of a training run.
    
    Args:
        run_id (int): ID of the training run
        status (str): New status ('running', 'completed', 'failed')
        metrics (dict, optional): Training metrics to save
    
    Returns:
        bool: True if update was successful
    """
    if status not in ['running', 'completed', 'failed']:
        raise ValueError("Status must be one of: running, completed, failed")
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Get existing metrics
    cursor.execute('SELECT metrics FROM training_runs WHERE id = ?', (run_id,))
    result = cursor.fetchone()
    
    if not result:
        conn.close()
        return False
    
    # Update metrics
    existing_metrics = json.loads(result['metrics'])
    if metrics:
        existing_metrics.update(metrics)
    
    # Update record
    cursor.execute('''
        UPDATE training_runs 
        SET status = ?, metrics = ?
        WHERE id = ?
    ''', (status, json.dumps(existing_metrics), run_id))
    
    success = cursor.rowcount > 0
    conn.commit()
    conn.close()
    
    return success

def add_training_examples(run_id: int, examples: List[Dict]) -> int:
    """
    Add training examples to a training run.
    
    Args:
        run_id (int): ID of the training run
        examples (list): List of example dictionaries with 'input', 'target', and optional 'type' and 'metadata'
    
    Returns:
        int: Number of examples added
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Prepare insertion
    count = 0
    for example in examples:
        if 'input' not in example or 'target' not in example:
            continue
        
        example_type = example.get('type', 'general')
        metadata = json.dumps(example.get('metadata', {}))
        
        cursor.execute('''
            INSERT INTO training_examples (
                run_id, input_text, target_text, example_type, metadata
            ) VALUES (?, ?, ?, ?, ?)
        ''', (run_id, example['input'], example['target'], example_type, metadata))
        
        count += 1
    
    conn.commit()
    conn.close()
    
    return count

def add_model_output(run_id: int, input_text: str, output_text: str, metadata: Optional[Dict] = None) -> int:
    """
    Add a model output to a training run.
    
    Args:
        run_id (int): ID of the training run
        input_text (str): Input prompt provided to the model
        output_text (str): Output generated by the model
        metadata (dict, optional): Additional metadata about the output
    
    Returns:
        int: ID of the new output entry
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    metadata_json = json.dumps(metadata or {})
    
    cursor.execute('''
        INSERT INTO model_outputs (
            run_id, input_text, output_text, timestamp, metadata
        ) VALUES (?, ?, ?, ?, ?)
    ''', (run_id, input_text, output_text, timestamp, metadata_json))
    
    output_id = cursor.lastrowid
    conn.commit()
    conn.close()
    
    if output_id is None:
        return -1  # Return a sentinel value if no ID was generated
    return output_id

def get_training_run(run_id: int) -> Optional[Dict]:
    """
    Get details of a specific training run.
    
    Args:
        run_id (int): ID of the training run
    
    Returns:
        dict: Training run details or None if not found
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT * FROM training_runs WHERE id = ?
    ''', (run_id,))
    
    result = cursor.fetchone()
    conn.close()
    
    if not result:
        return None
    
    # Convert to dictionary with parsed JSON fields
    run = dict(result)
    run['hyperparameters'] = json.loads(run['hyperparameters'])
    run['metrics'] = json.loads(run['metrics'])
    
    return run

def get_all_training_runs() -> List[Dict]:
    """
    Get a list of all training runs.
    
    Returns:
        list: List of training run summaries
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT id, model_name, base_model, status, timestamp 
        FROM training_runs
        ORDER BY timestamp DESC
    ''')
    
    runs = [dict(row) for row in cursor.fetchall()]
    conn.close()
    
    return runs

def get_training_examples(run_id: Optional[int] = None, example_type: Optional[str] = None, limit: Optional[int] = None) -> List[Dict]:
    """
    Get training examples, optionally filtered by run_id and type.
    
    Args:
        run_id (int, optional): Filter by training run ID
        example_type (str, optional): Filter by example type
        limit (int, optional): Limit the number of examples returned
    
    Returns:
        list: List of training examples
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    query = 'SELECT * FROM training_examples'
    params = []
    
    # Build WHERE clause based on filters
    where_clauses = []
    if run_id is not None:
        where_clauses.append('run_id = ?')
        params.append(run_id)
    
    if example_type is not None:
        where_clauses.append('example_type = ?')
        params.append(example_type)
    
    if where_clauses:
        query += ' WHERE ' + ' AND '.join(where_clauses)
    
    query += ' ORDER BY id'
    
    if limit is not None:
        query += ' LIMIT ?'
        params.append(limit)
    
    cursor.execute(query, params)
    
    # Convert to list of dictionaries with parsed metadata
    examples = []
    for row in cursor.fetchall():
        example = dict(row)
        example['metadata'] = json.loads(example['metadata'])
        examples.append(example)
    
    conn.close()
    
    return examples

def log_page(title: str, page_id: str, revision_id: str, content_hash: str, action: str = "added", 
             is_revision: bool = False, parent_id: Optional[str] = None, 
             revision_number: Optional[int] = None) -> int:
    """
    Log a Wikipedia page entry in the database.
    
    Args:
        title (str): Page title
        page_id (str): Wikipedia page ID
        revision_id (str): Wikipedia revision ID
        content_hash (str): Hash of page content
        action (str, optional): Operation type (added/updated/removed)
        is_revision (bool, optional): Whether this entry is a revision of another page
        parent_id (str, optional): ID of the parent page if this is a revision
        revision_number (int, optional): Revision number if this is a revision
    
    Returns:
        int: ID of the inserted entry
    """
    if action not in ["added", "updated", "removed"]:
        raise ValueError("Action must be one of: added, updated, removed")
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    
    try:
        cursor.execute('''
            INSERT INTO entries (
                title, page_id, revision_id, timestamp, content_hash,
                action, is_revision, parent_id, revision_number
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            title, page_id, revision_id, timestamp, content_hash,
            action, is_revision, parent_id, revision_number
        ))
        
        entry_id = cursor.lastrowid
        if entry_id is None:
            conn.rollback()
            return -1  # Return a sentinel value if no ID was generated
        
        # Create initial training metadata record
        cursor.execute('''
            INSERT INTO training_metadata (entry_id, used_in_training)
            VALUES (?, 0)
        ''', (entry_id,))
        
        conn.commit()
        return entry_id
    
    except sqlite3.IntegrityError:
        # Handle case where page_id already exists
        if action == "updated":
            # Update existing entry
            cursor.execute('''
                UPDATE entries
                SET revision_id = ?, timestamp = ?, content_hash = ?, action = ?
                WHERE page_id = ?
            ''', (revision_id, timestamp, content_hash, action, page_id))
            
            # Get updated entry_id
            cursor.execute('SELECT id FROM entries WHERE page_id = ?', (page_id,))
            entry_id = cursor.fetchone()['id']
            conn.commit()
            return entry_id
        else:
            # For other actions, just return existing entry ID
            cursor.execute('SELECT id FROM entries WHERE page_id = ?', (page_id,))
            entry_id = cursor.fetchone()['id']
            return entry_id
    finally:
        conn.close()

def mark_used_in_training(page_ids: List[str], model_checkpoint: str, 
                          training_metrics: Optional[Dict[str, Dict[str, Any]]] = None) -> None:
    """
    Mark pages as used in training with associated model checkpoint and metrics.
    
    Args:
        page_ids (List[str]): List of page IDs used in training
        model_checkpoint (str): Hash or identifier of the model checkpoint
        training_metrics (Dict, optional): Dictionary mapping page_ids to their training metrics
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    
    for page_id in page_ids:
        # Get entry ID for this page
        cursor.execute('SELECT id FROM entries WHERE page_id = ?', (page_id,))
        entry_row = cursor.fetchone()
        
        if not entry_row:
            continue
        
        entry_id = entry_row['id']
        
        # Get the training metadata ID
        cursor.execute('SELECT id FROM training_metadata WHERE entry_id = ?', (entry_id,))
        metadata_row = cursor.fetchone()
        
        if not metadata_row:
            # Create a new metadata record if none exists
            cursor.execute('''
                INSERT INTO training_metadata (
                    entry_id, used_in_training, training_timestamp, model_checkpoint
                ) VALUES (?, 1, ?, ?)
            ''', (entry_id, timestamp, model_checkpoint))
            metadata_id = cursor.lastrowid
            if metadata_id is None:
                # If we couldn't get an ID, use a default (this shouldn't happen)
                conn.rollback()
                continue
        else:
            metadata_id = metadata_row['id']
            # Update existing metadata
            cursor.execute('''
                UPDATE training_metadata
                SET used_in_training = 1, training_timestamp = ?, model_checkpoint = ?
                WHERE id = ?
            ''', (timestamp, model_checkpoint, metadata_id))
        
        # Add metrics if provided
        if training_metrics and page_id in training_metrics:
            page_metrics = training_metrics[page_id]
            
            # Update loss metrics
            if 'average_loss' in page_metrics and 'relative_loss' in page_metrics:
                # Get loss values with default of 0.0 if not present
                avg_loss = page_metrics.get('average_loss')
                rel_loss = page_metrics.get('relative_loss')
                
                # Convert to float with safe defaults
                avg_loss_float = 0.0 if avg_loss is None else float(avg_loss)
                rel_loss_float = 0.0 if rel_loss is None else float(rel_loss)
                
                cursor.execute('''
                    UPDATE training_metadata
                    SET average_loss = ?, relative_loss = ?
                    WHERE id = ?
                ''', (
                    avg_loss_float,
                    rel_loss_float,
                    metadata_id
                ))
            
            # Handle token impact data
            token_impact = page_metrics.get('token_impact')
            if token_impact and isinstance(token_impact, dict):
                # Create token impact record
                cursor.execute('''
                    INSERT INTO token_impacts (metadata_id, total_tokens)
                    VALUES (?, ?)
                ''', (metadata_id, token_impact.get('total_tokens', 0)))
                
                token_impact_id = cursor.lastrowid
                
                # Add top tokens
                top_tokens = token_impact.get('top_tokens', [])
                for token in top_tokens:
                    position = token['position']
                    context_start = token.get('context', [position, position])[0]
                    context_end = token.get('context', [position, position])[1]
                    
                    cursor.execute('''
                        INSERT INTO top_tokens (
                            token_impact_id, token_id, position, impact,
                            context_start, context_end
                        ) VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        token_impact_id,
                        token['token_id'],
                        position,
                        float(token['impact']),
                        context_start,
                        context_end
                    ))
    
    conn.commit()
    conn.close()

def get_page_by_id(page_id: str) -> Optional[Dict]:
    """
    Get a single page by its ID.
    
    Args:
        page_id (str): The page ID to look up
        
    Returns:
        Dict or None: The page entry if found
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT e.*, tm.used_in_training, tm.training_timestamp, 
               tm.model_checkpoint, tm.average_loss, tm.relative_loss
        FROM entries e
        LEFT JOIN training_metadata tm ON e.id = tm.entry_id
        WHERE e.page_id = ?
    ''', (page_id,))
    
    row = cursor.fetchone()
    conn.close()
    
    if not row:
        return None
    
    return dict(row)

def get_page_history(page_id: str) -> List[Dict]:
    """
    Get all entries for a specific page ID.
    
    Args:
        page_id (str): Wikipedia page ID
        
    Returns:
        List[Dict]: List of page entries
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                SELECT e.*, tm.used_in_training, tm.training_timestamp, 
                       tm.model_checkpoint, tm.average_loss, tm.relative_loss
                FROM entries e
                LEFT JOIN training_metadata tm ON e.id = tm.entry_id
                WHERE e.page_id = ?
                ORDER BY e.timestamp DESC
            ''', (page_id,))
            
            entries = []
            for row in cursor.fetchall():
                try:
                    entries.append(dict(row))
                except Exception as e:
                    print(f"Error converting row to dict: {str(e)}")
                    # Skip this row and continue
                    continue
        except Exception as e:
            print(f"Error querying database: {str(e)}")
            return []
        finally:
            conn.close()
        
        return entries
    except Exception as e:
        print(f"Unexpected error in get_page_history: {str(e)}")
        return []

def check_updates(page_id: str, revision_id: str) -> bool:
    """
    Check if a page needs updating based on revision ID.
    
    Args:
        page_id (str): Wikipedia page ID
        revision_id (str): Current revision ID to check
        
    Returns:
        bool: True if page needs updating, False otherwise
    """
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"Checking updates for page_id={page_id}, revision_id={revision_id}")
        
        # First check if the page exists at all
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            # Use a simple count query first to check existence
            cursor.execute('SELECT COUNT(*) FROM entries WHERE page_id = ?', (page_id,))
            count = cursor.fetchone()[0]
            
            if count == 0:
                logger.info(f"No existing entry found for page_id={page_id}, needs adding")
                conn.close()
                return True  # No existing entry, needs adding
            
            # If we get here, the page exists, so get its revision_id
            cursor.execute('''
                SELECT revision_id 
                FROM entries 
                WHERE page_id = ? 
                ORDER BY timestamp DESC
                LIMIT 1
            ''', (page_id,))
            
            result = cursor.fetchone()
            logger.info(f"Found existing entry with page_id={page_id}")
            
            if not result:
                logger.warning(f"Strange: count was {count} but no result found")
                return True
            
            # Get the revision_id as bytes and decode it safely
            db_revision_id_bytes = result[0]
            if isinstance(db_revision_id_bytes, bytes):
                db_revision_id = db_revision_id_bytes.decode('utf-8', errors='replace')
            else:
                db_revision_id = str(db_revision_id_bytes)
            
            logger.info(f"Comparing revision IDs: {db_revision_id} vs {revision_id}")
            return db_revision_id != revision_id
            
        except Exception as e:
            logger.error(f"Error querying database: {str(e)}")
            logger.error(f"Exception type: {type(e).__name__}")
            # If there's an error, assume we need to add the page
            return True
        finally:
            conn.close()
            
    except Exception as e:
        logger.error(f"Unexpected error in check_updates: {str(e)}")
        logger.error(f"Exception type: {type(e).__name__}")
        # If there's an error, assume we need to add the page
        return True

def get_unused_pages() -> List[Dict]:
    """
    Get all pages that haven't been used in training.
    
    Returns:
        List[Dict]: List of page entries that haven't been used in training
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT e.*
        FROM entries e
        JOIN training_metadata tm ON e.id = tm.entry_id
        WHERE tm.used_in_training = 0
    ''')
    
    entries = [dict(row) for row in cursor.fetchall()]
    conn.close()
    
    return entries

def get_page_revisions(page_id: str) -> List[Dict]:
    """
    Get all revision entries for a page.
    
    Args:
        page_id (str): Wikipedia page ID
        
    Returns:
        List[Dict]: List of revision entries for the page, sorted by revision_number
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT e.*, tm.used_in_training, tm.training_timestamp, 
               tm.model_checkpoint, tm.average_loss, tm.relative_loss
        FROM entries e
        LEFT JOIN training_metadata tm ON e.id = tm.entry_id
        WHERE e.is_revision = 1 AND e.parent_id = ?
        ORDER BY e.revision_number
    ''', (page_id,))
    
    revisions = [dict(row) for row in cursor.fetchall()]
    conn.close()
    
    return revisions

def get_main_pages() -> List[Dict]:
    """
    Get all non-revision pages.
    
    Returns:
        List[Dict]: List of main page entries (excluding revisions)
    """
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Use a simpler query without the problematic columns
        query = '''
            SELECT e.*, tm.used_in_training
            FROM entries e
            LEFT JOIN training_metadata tm ON e.id = tm.entry_id
            WHERE e.is_revision = 0
        '''
        
        logger.info(f"Executing query: {query}")
        cursor.execute(query)
        
        pages = []
        for row in cursor.fetchall():
            page = dict(row)
            # If training_metadata values are NULL or missing, set defaults
            if page.get("used_in_training") is None:
                page["used_in_training"] = 0
            
            # Add missing columns with default values
            if "training_timestamp" not in page:
                page["training_timestamp"] = None
            if "model_checkpoint" not in page:
                page["model_checkpoint"] = None
            if "average_loss" not in page:
                page["average_loss"] = None
            if "relative_loss" not in page:
                page["relative_loss"] = None
            pages.append(page)
        conn.close()
        
        return pages
    except Exception as e:
        logger.error(f"Error in get_main_pages: {str(e)}")
        logger.error(f"Exception type: {type(e).__name__}")
        # Return an empty list to avoid breaking the caller
        return []

def remove_unused_entries() -> int:
    """
    Remove all entries that haven't been used in training.
    
    Returns:
        int: Number of entries removed
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # First, delete related training_metadata entries
        cursor.execute('''
            DELETE FROM training_metadata
            WHERE entry_id IN (
                SELECT e.id
                FROM entries e
                JOIN training_metadata tm ON e.id = tm.entry_id
                WHERE tm.used_in_training = 0
            )
        ''')
        
        # Then delete the entries
        cursor.execute('''
            DELETE FROM entries
            WHERE id IN (
                SELECT e.id
                FROM entries e
                JOIN training_metadata tm ON e.id = tm.entry_id
                WHERE tm.used_in_training = 0
            )
        ''')
        
        count = cursor.rowcount
        conn.commit()
        return count
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Error removing unused entries: {str(e)}")
        conn.rollback()
        return 0
    finally:
        conn.close()

def export_to_json(output_path: str = "changelog_export.json") -> bool:
    """
    Export the entire database to a JSON file similar to the original changelog.json format.
    
    Args:
        output_path (str): Path to save the JSON file
        
    Returns:
        bool: True if export was successful
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Get all entries with their training metadata
    cursor.execute('''
        SELECT e.*, tm.used_in_training, tm.training_timestamp, 
               tm.model_checkpoint, tm.average_loss, tm.relative_loss,
               tm.id as metadata_id
        FROM entries e
        LEFT JOIN training_metadata tm ON e.id = tm.entry_id
    ''')
    
    entries = []
    for row in cursor.fetchall():
        entry = dict(row)
        metadata_id = entry.pop('metadata_id')
        
        # Create training_metadata structure
        training_metadata = {
            "used_in_training": entry.pop('used_in_training'),
            "training_timestamp": entry.pop('training_timestamp'),
            "model_checkpoint": entry.pop('model_checkpoint'),
            "average_loss": entry.pop('average_loss'),
            "relative_loss": entry.pop('relative_loss'),
            "token_impact": None
        }
        
        # Get token impact data if it exists
        if metadata_id is not None:
            cursor.execute('''
                SELECT id, total_tokens
                FROM token_impacts
                WHERE metadata_id = ?
            ''', (metadata_id,))
            
            token_impact_row = cursor.fetchone()
            if token_impact_row:
                token_impact_id = token_impact_row['id']
                total_tokens = token_impact_row['total_tokens']
                
                # Get top tokens
                cursor.execute('''
                    SELECT token_id, position, impact, context_start, context_end
                    FROM top_tokens
                    WHERE token_impact_id = ?
                ''', (token_impact_id,))
                
                top_tokens = []
                for token_row in cursor.fetchall():
                    top_tokens.append({
                        "token_id": token_row['token_id'],
                        "position": token_row['position'],
                        "impact": token_row['impact'],
                        "context": [token_row['context_start'], token_row['context_end']]
                    })
                
                training_metadata["token_impact"] = {
                    "top_tokens": top_tokens,
                    "total_tokens": total_tokens
                }
        
        entry["training_metadata"] = training_metadata
        entries.append(entry)
    
    conn.close()
    
    # Create output JSON
    json_data = {"entries": entries}
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        return True
    except Exception:
        return False
